{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e104077d",
   "metadata": {},
   "source": [
    "# clEsperanto: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0136a9",
   "metadata": {},
   "source": [
    "The python package `pyclesperanto` can be install using `pip` or `mamba`. If all went well, there shouldn't be any issue at the package import stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyclesperanto as cle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "\n",
    "print(f\"we are using pyclesperanto version {cle.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46722a3",
   "metadata": {},
   "source": [
    "`clEsperanto` runs on __OpenCL__ (Open Computing Language) which is a standard language for parallel programming of diverse Processing Units which can be Graphical (GPU) or Central (CPU). Strong point of the language is its compatibility with a vast set of devices. At import, clesperanto will automatically select a device (the first one found) but it might not be the best nor the one you want to select is you have more than one device.\n",
    "\n",
    "Hence, the first step is to prospect your hardware and identify the best device to run our operations. We provide the following function to enquiry and manage the Processing Units of your system.\n",
    "\n",
    "### Exercice 1: System specification\n",
    "\n",
    "Using `cle.info()` fetch your full system specification: How many devices available do you have? Which device is the most adapted for you?\n",
    "\n",
    "Here are a few quick definition:\n",
    "\n",
    "- __GLOBAL_MEM_SIZE__: Total RAM memory of the device\n",
    "- __MAX_MEM_ALLOC_SIZE__: Maximum RAM memory allocation possible\n",
    "- __MAX_COMPUTE_UNITS__: Number of computing core of the Processing Unit\n",
    "- __MAX_CLOCK_FREQUENCY__: Processing speed of each core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Check your system information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08069768",
   "metadata": {},
   "source": [
    "### Exercice 2: Select a device\n",
    "\n",
    "Using `cle.select_device()`, select a specific device from your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - select a specific device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ec4e7",
   "metadata": {},
   "source": [
    "The devices memory and main computer memory are separated, in order for a device to acces a data it requires to transfert it from the main memory to the device memory, and to transfert it back to the main computer memory once all processing are done. \n",
    "\n",
    "Because of this, `clesperanto` comes with its own array structure which is compatible with `numpy.array`, and a set of function to send and read from the device:\n",
    "- `push()` : send an array from CPU to GPU\n",
    "- `pull()` : read an array from GPU to CPU\n",
    "- `create()` : allocate empty space on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np_arr = np.random.rand(5,10)  # CPU array\n",
    "gpu_arr = cle.push(np_arr)    # GPU array\n",
    "out_arr = cle.pull(gpu_arr)   # CPU array\n",
    "\n",
    "print(f\"np_arr:  shape={np_arr.shape}, dtype={np_arr.dtype}, device={np_arr.device}, type={type(np_arr)}\")\n",
    "print(f\"gpu_arr: shape={gpu_arr.shape}, dtype={gpu_arr.dtype}, device={gpu_arr.device.name}, type={type(gpu_arr)}\")\n",
    "print(f\"out_arr: shape={out_arr.shape}, dtype={out_arr.dtype}, device={out_arr.device}, type={type(out_arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a672192",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ngpu arr(0,0)=\", gpu_arr[0,0])  # Accessing element at (0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89315076",
   "metadata": {},
   "source": [
    "### Exercice 1: Load an image into your device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imread('./data/blobs.png').squeeze()\n",
    "\n",
    "# TODO - check the image shape and push it to the GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd13eb27",
   "metadata": {},
   "source": [
    "### Exercise 2: What is the largest array you can push to your device?\n",
    "\n",
    "One of the biggest limitation in GPU-acceleration is the memory limitation of your device, what is the size data you can push to your device? Does it fit your hardware specification?  \n",
    "Here, we want to see your hardware limitation and the type of error you would get if this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92106951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - generate a large random numpy array and push it to the GPU to observe the memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a806bda",
   "metadata": {},
   "source": [
    "You can trace your device usage with various OSs application to see memory occupancy and the core usage:\n",
    "- MacOS: Activity Monitor > View > GPU History\n",
    "- Windows: Task Manager > Performance\n",
    "- NVIDIA: run `watch -n0.1 nvidia-smi` in a prompt / terminal\n",
    "- ...\n",
    "\n",
    "When not used anymore, the memory is free by the garbage colector. However, you may want to free yourself some memory to optise your code. This can be done using the `del` keyword of python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - delete some GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1212c1a",
   "metadata": {},
   "source": [
    "### Array arythmetics and manipulation\n",
    "\n",
    "clesperanto Array aim to provide similar behaviour as Numpy Array, with some limitation of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cle.push(imread('./data/blobs.png').squeeze())\n",
    "normalized = image / image.max()\n",
    "hole = cle.copy(image)\n",
    "hole[25:75, 25:75] = 1\n",
    "binarized = image >= 100 \n",
    "\n",
    "print(f\"max intensity: {image.max()}, min intensity: {image.min()}\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,15))\n",
    "axs[0].imshow(normalized)\n",
    "axs[0].set_title(f\"Normalized {normalized.dtype}\")\n",
    "axs[1].imshow(hole)\n",
    "axs[1].set_title(f\"Hole {hole.dtype}\")\n",
    "axs[2].imshow(binarized)\n",
    "axs[2].set_title(f\"Binarized {binarized.dtype}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f04254",
   "metadata": {},
   "source": [
    "clEsperanto is a typed library, with int, unsigned int of 8, 16, and 32 bit as well as float of 32 bits. Unless specified by the user, the return output buffer is by default the same type as the input buffer or of a predifined type for some specific algorithm (e.g. `gaussian_blur` return `float32` by default, `threshold_otsu` return `uint8` by default)\n",
    "\n",
    "### Exercise 3: update the cell above to fix the normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea532575",
   "metadata": {},
   "source": [
    "# Cupy: Introduction\n",
    "\n",
    "> __WARNING__ Only for NVIDIA Hardware !\n",
    "\n",
    "Cupy is a library for processing data using __CUDA__ (Compute Unified Device Architecture) a proprietary language for NVIDIA graphics cards __only__.\n",
    "\n",
    "It is a numpy/scipy compatible library for GPU acceleration and can act as a dropin replacement, with a seemless integration to the __numpy__ ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf008ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cupy as xp\n",
    "except:\n",
    "    import numpy as xp\n",
    "    Warning(\"Cupy not found, using numpy instead.\")\n",
    "\n",
    "try:\n",
    "    import cupyx.scipy.ndimage as xdi\n",
    "except:\n",
    "    import scipy.ndimage as xdi\n",
    "    Warning(\"Cupy not found, using scipy instead.\")\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "print(xp)\n",
    "print(xdi)\n",
    "print()\n",
    "print(np)\n",
    "print(ndi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c108a83",
   "metadata": {},
   "source": [
    "Because it is CUDA based, only NVIDIA device can be used. Hence, you can identify the hardware you want to use based on their __index__ which is based on the library discovery pattern which can change vary between system and version.\n",
    "\n",
    "We can explore the devices specificities and we should recover similar information than with `clesperanto` with possible different name convention or units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ce526",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = xp.cuda.runtime.getDeviceCount()\n",
    "for i in range(num_devices):\n",
    "    print(f\"Device {i}:\")\n",
    "    device_properties = xp.cuda.runtime.getDeviceProperties(i)\n",
    "    for k, v in device_properties.items():\n",
    "        print(f\"- {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798ff5b",
   "metadata": {},
   "source": [
    "Even if the interface allows dropin-replacement, the memory still requires a copy from the CPU memory to the GPU memory. Here, Cupy rely on a numpy style api with `asarray` and `get` methods to transfert the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np_arr = np.random.rand(5,10)  # CPU array\n",
    "gpu_arr = xp.asarray(np_arr)   # GPU array\n",
    "out_arr = gpu_arr.get()        # CPU array\n",
    "\n",
    "print(f\"np_arr:  shape={np_arr.shape}, dtype={np_arr.dtype}, device={np_arr.device}, type={type(np_arr)}\")\n",
    "print(f\"gpu_arr: shape={gpu_arr.shape}, dtype={gpu_arr.dtype}, device={gpu_arr.device}, type={type(gpu_arr)}\")\n",
    "print(f\"out_arr: shape={out_arr.shape}, dtype={out_arr.dtype}, device={out_arr.device}, type={type(out_arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ngpu arr(0,0)=\", gpu_arr[0,0])  # Accessing element at (0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45270084",
   "metadata": {},
   "source": [
    "### Exercise 4: What append if I pass a cupy array to clesperanto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
